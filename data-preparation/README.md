Of course. Here is a comprehensive documentation for the end-to-end dataset preparation pipeline, written to explain the "what" and "why" behind the process for an internal team.

---

# Documentation: Automated Dataset Preparation Pipeline for TTT-Video-DiT

## 1. Introduction & Core Concepts

This document details the automated pipeline for preparing a video dataset suitable for training the TTT-Video-DiT model. Before diving into the steps, it's crucial to understand *why* this specific data structure is necessary.

### What is TTT-Video-DiT?

TTT-Video is a Diffusion Transformer (DiT) model designed to generate long, coherent videos (e.g., 60+ seconds). Standard video models often struggle with long-term consistency, where objects or environments change unnaturally over time.

TTT-Video solves this by processing video in two distinct ways simultaneously:

1.  **Local Attention:** The model's standard Transformer blocks focus on a short, local window of video at a time—specifically, a **3-second segment**. This allows it to generate high-fidelity motion and detail within that small window.
2.  **Global Context (TTT Layer):** A special Test-Time Training (TTT) layer operates across *all* the 3-second segments. This layer acts as a "memory," carrying information from past segments into the future to ensure that characters, outfits, and backgrounds remain consistent throughout the entire video.

### Why This Data Preparation is Crucial

Because the model thinks in **3-second atomic units**, our entire dataset must be built from these blocks. We cannot simply feed the model a 60-second video. Instead, we must:

1.  **Deconstruct** our source videos into these precise 3-second segments.
2.  **Provide a unique, detailed text annotation for *each* 3-second segment.** This ensures the model learns a tight alignment between a specific action and its description.
3.  **Preserve Scene Information:** We must tell the model when a new scene begins or ends so it can use special tokens to handle the transition, preventing jarring cuts.
4.  **Create "Recipes" for Long Videos:** For training longer contexts (e.g., 9s, 18s), we don't physically merge videos. We create metadata files (`.jsonl`) that act as recipes, telling the dataloader which consecutive 3-second segments to load and stitch together on-the-fly.

This pipeline automates this entire complex process, from raw video episodes to the final training-ready metadata files.

## 2. Prerequisites

Before starting, ensure the following are completed:

1.  **Hardware & Software:**
    *   A Python 3 environment.
    *   `ffmpeg` installed and accessible from your system's PATH.
    *   `git` installed for cloning repositories.

2.  **Accounts & Keys:**
    *   A **Google Gemini API Key**. You can obtain one from [Google AI Studio](https://aistudio.google.com/app/apikey).

3.  **Code & Pre-trained Models:**
    *   A local clone of the official `ttt-video-dit` repository.
    *   **Converted CogVideoX Weights:** You **must** first run the `scripts/convert_weights_from_hf.sh` script from the official repository. This downloads and converts the VAE, T5, and Transformer models into the required format. Note the path to this converted directory.

## 3. Setup Instructions

1.  **Directory Structure:**
    Create the following directory structure for the pipeline.

    ```
    video_dataset_pipeline/
    ├── 01_gemini_annotate.py
    ├── 02_ffmpeg_process.py
    ├── 03_run_precomputation.py
    ├── 04_assemble_metadata.py
    ├── config.py
    ├── requirements.txt
    ├── data/
    │   ├── raw_videos/
    │   │   └── episode_001.mp4  # <-- Place your raw episode videos here
    │   └── processing/          # <-- All intermediate files will be generated here
    └── README.md
    ```

2.  **Install Dependencies:**
    Install the required Python libraries.

    ```bash
    pip install -r requirements.txt
    # Also install dependencies for the TTT codebase from its own requirements file.
    ```

3.  **Configuration:**
    Open `config.py` and edit the user configuration section. **This is a mandatory step.**

    *   `GEMINI_API_KEY`: Your API key for the Gemini SDK.
    *   `TTT_CODEBASE_PATH`: The absolute path to your local clone of the `ttt-video-dit` repository.
    *   `CONVERTED_COGVIDEOX_DIR`: The absolute path to the directory containing the model weights generated by the `convert_weights_from_hf.sh` script.

---
## 4. End-to-End Pipeline Execution

Run the following scripts sequentially.

### **Step 1: AI-Powered Annotation (`01_gemini_annotate.py`)**

*   **Purpose:** To automate the creation of detailed, per-segment annotations. This script uses a two-pass Gemini strategy for maximum accuracy.
*   **Input:** Raw video files located in `data/raw_videos/`.
*   **Output:** A detailed JSON file for each video (e.g., `episode_001.json`) saved in `data/processing/1_annotations/`.

#### **Gemini Prompting Strategy**

1.  **Pass 1: Scene Boundary Detection:** The script first analyzes the entire video to identify high-level scene boundaries. This is crucial for later applying `scene_start` and `scene_end` tokens.
    > **Prompt for Scene Detection:**
    > ```
    > Analyze this video and identify all distinct scenes. A new scene is defined by a significant change in location, background, time of day, or a major shift in narrative focus. Provide a list of scenes with precise start and end timestamps in 'HH:MM:SS.ms' format.
    >
    > Your output **must** be a valid JSON object with a single key "scenes" containing a list of objects...
    > ```

2.  **Pass 2: Granular 3-Second Annotation:** The script then iterates through each detected scene in 3-second intervals and prompts Gemini for a description of *only that specific window*. This ensures every training clip has a perfectly aligned text description.
    > **Prompt for 3-Second Segment:**
    > ```
    > Focus **only** on the 3-second video segment from {segment_start_time} to {segment_end_time}.
    > Provide a detailed, descriptive paragraph of 2-3 sentences describing the primary action within this short clip.
    > Describe the main character's action, any significant object interactions, and camera movement (e.g., "camera pans left"). Do not describe events before or after this specific time window.
    > ```

*   **Command:**
    ```bash
    python 01_gemini_annotate.py
    ```

### **Step 2: Automated Video Clipping (`02_ffmpeg_process.py`)**

*   **Purpose:** To programmatically process the raw videos based on the structured data from Gemini.
*   **Process:**
    1.  Reads the scene timestamps and crops each full scene to be a perfect multiple of 3 seconds.
    2.  Splits each cropped scene into consecutive 3-second video clips.
*   **Input:** Raw videos and the granular annotation JSONs from Step 1.
*   **Output:** Individual 3-second video clips (`.mp4`) organized by episode in `data/processing/3_segments_3s/`.
*   **Command:**
    ```bash
    python 02_ffmpeg_process.py
    ```

### **Step 3: Official Embedding Generation (`03_run_precomputation.py`)**

*   **Purpose:** To generate the VAE and T5 embeddings using the official scripts provided by the TTT authors. This pipeline script prepares the inputs and calls those scripts.
*   **Input:** The 3-second video clips from Step 2 and the granular annotations from Step 1.
*   **Output:**
    *   VAE-encoded video latents (`.pth`) in `data/processing/5_precomp_output/video_latents/`.
    *   T5-encoded text embeddings (`.pt`) for each annotation in `data/processing/5_precomp_output/text_embeddings/`.
*   **Command:**
    ```bash
    python 03_run_precomputation.py
    ```

### **Step 4: Final Metadata Assembly (`04_assemble_metadata.py`)**

*   **Purpose:** To create the final `metadata.jsonl` files that the training dataloader will consume.
*   **Input:** The precomputed video latents and text embeddings from Step 3.
*   **Output:** `metadata_Xs.jsonl` files (e.g., `metadata_3s.jsonl`, `metadata_9s.jsonl`) saved in `data/processing/6_final_metadata/`.
*   **Command:**
    ```bash
    python 04_assemble_metadata.py
    ```

#### **How the Metadata Works**

The generated `metadata_Xs.jsonl` files **do not contain video or text data**. Instead, each line is a **recipe** telling the dataloader how to build a training sample on-the-fly.

For example, a single line in `metadata_18s.jsonl` will contain:
*   A list of **six** paths to consecutive 3-second video latent files.
*   A list of **six** paths to the corresponding 3-second text embedding files.

During training, the `VideoDataset` reads this recipe, loads the six specified latent files, loads the six text embedding files, and concatenates them in memory to create a single 18-second training sample. This approach is highly efficient as it avoids duplicating data on disk.

## Final Output & Next Steps

Upon successful completion of all four scripts, the `data/processing/6_final_metadata/` directory will contain all the necessary metadata files to begin training. You can now proceed with the 5-stage training by pointing the `training.jsonl_paths` variable in your TOML configuration file to the appropriate `metadata_Xs.jsonl` file for each stage.